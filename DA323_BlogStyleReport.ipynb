{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11736359,"sourceType":"datasetVersion","datasetId":7367831}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Seeing the Feelings: Multimodal Sentiment Analysis Using Image and Text  \n### Karnati Ravi Teja\n### 220150006","metadata":{}},{"cell_type":"markdown","source":"## Motivation\n\nUnderstanding human sentiment through text alone can be tricky — it’s easy to miss things like sarcasm, ambiguity, or subtle emotional cues. That’s where images come in. When text is paired with visuals (like in memes, reviews, or social media posts), they add extra context that can make the sentiment clearer.\n\nThis project focuses on **Multimodal Sentiment Analysis** — combining both image and text inputs to get a more accurate read on emotions. I chose this topic because it brings together computer vision and language understanding in a way that feels both practical and impactful. \n","metadata":{}},{"cell_type":"markdown","source":"## Multimodal Learning: A Quick Look Back\n\nMultimodal learning is all about combining different types of data — like text, images, or audio — to build smarter models. Early approaches were pretty straightforward — just stack features from each modality together.\n\nBut modern models go way beyond that. With techniques like **attention mechanisms** and **transformers**, they actually learn how different modalities interact.\n\n### Some popular models in this space:\n- **ViLBERT**: Basically BERT which also understands images \n- **LXMERT**: Uses co-attention to deeply connect image and text features\n- **CLIP**: Trains on huge image–text datasets to map both into the same embedding space\n\nFor this project, I’m implementing a simplified version of these ideas — using attention-based fusion to combine image and text signals more effectively.\n","metadata":{}},{"cell_type":"markdown","source":"## What I Learned From This Work\n\nThis project really helped me understand how **multimodal models** work in practice. Some key takeaways:\n\n- I learned how separate encoders (like **ResNet** for images and **BERT** for text) can be combined for joint prediction tasks.\n- I explored **attention fusion**, where features from different modalities don’t just get merged — they actually interact and highlight relevant parts in each other.\n- I now get how **cross-modal attention** helps align visual cues with language, making predictions more accurate — especially when one modality is vague or noisy.\n- Working with **pretrained models** showed me how powerful transfer learning can be, even across different types of data.\n\nOverall, it felt good to go beyond theory and see how deep learning + attention mechanisms come together in the real world.","metadata":{}},{"cell_type":"markdown","source":"## How Attention Fusion Works\n\n- First, text and images are encoded separately.\n- **Attention fusion** helps the **text focus on relevant image parts**, and vice versa.\n- This is done using **cross-attention**:\n  - Each word in the sentence \"asks\" which image regions are important.\n  - Each image patch \"asks\" which words are meaningful.\n- The result? **Context-aware embeddings** that are fused together and then classified.\n\nThis approach beats simple concatenation because it allows the model to learn **interactions between modalities** instead of treating them as separate, independent pieces.\n","metadata":{}},{"cell_type":"markdown","source":"### 1. **Text Classifier Model**\nThe **Text Classifier** uses **word embeddings** and an **LSTM** network to process text data. The text features are obtained through the pre-trained embedding matrix and passed through a bidirectional LSTM layer to capture contextual information.\n","metadata":{}},{"cell_type":"code","source":"class TextClassifier(nn.Module):\n    def __init__(self):\n        super(TextClassifier, self).__init__()\n        vocab_size, embedding_dim = EMBEDDING_MATRIX.shape\n        self.embedding = nn.Embedding.from_pretrained(torch.tensor(EMBEDDING_MATRIX), freeze=True)\n        self.dropout = nn.Dropout(0.5)\n        self.lstm1 = nn.LSTM(embedding_dim, HIDDEN_SIZE, batch_first=True, bidirectional=True)\n\n    def forward(self, x):\n        embedded = self.embedding(x)\n        embedded = self.dropout(embedded)\n        output1, _ = self.lstm1(embedded)\n        last_hidden_state = torch.cat((output1[:, -1, :HIDDEN_SIZE], output1[:, 0, HIDDEN_SIZE:]), dim=1)\n        return last_hidden_state","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2. **Multimodal Classifier Model**\nThe Multimodal Classifier takes input from both the text and image models. It fuses the features using attention-based fusion (early fusion can also be applied depending on your experiment) and then classifies the multimodal features into different sentiment classes.","metadata":{}},{"cell_type":"markdown","source":"Attention-based fusion","metadata":{}},{"cell_type":"code","source":"class MultimodalClassifier(nn.Module):\n    def __init__(self, text_model, image_model):\n        super(MultimodalClassifier, self).__init__()\n        self.text_model = text_model\n        self.image_model = image_model\n        self.attention_weights = nn.Parameter(torch.Tensor(1, 1, 2))\n        self.fc = nn.Linear(HIDDEN_SIZE * 2 + 256, NUM_CLASSES)\n\n    def forward(self, text_input, image_input):\n        # Extract text features\n        text_features = self.text_model(text_input)  # Shape: [batch_size, HIDDEN_SIZE * 2]\n\n        # Extract image features\n        image_features = self.image_model(image_input)  # Shape: [batch_size, 256]\n\n        # Apply attention to text features\n        attention_scores = torch.matmul(text_features.unsqueeze(2), self.attention_weights)  # Shape: [batch_size, HIDDEN_SIZE * 2, 1]\n        attention_weights = torch.softmax(attention_scores, dim=1)  # Shape: [batch_size, HIDDEN_SIZE * 2, 1]\n        attended_text_features = (text_features * attention_weights.squeeze(2)).sum(dim=1)  # Shape: [batch_size, HIDDEN_SIZE * 2]\n\n        # Concatenate text features and image features\n        multimodal_features = torch.cat((attended_text_features, image_features), dim=1)  # Shape: [batch_size, HIDDEN_SIZE * 2 + 256]\n\n        # Classify the multimodal features\n        logits = self.fc(multimodal_features)  # Shape: [batch_size, NUM_CLASSES]\n        return logits\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Early Fusion (Optional):","metadata":{}},{"cell_type":"code","source":"class EarlyFusionMultimodalClassifier(nn.Module):\n    def __init__(self, text_model, image_model):\n        super(EarlyFusionMultimodalClassifier, self).__init__()\n        self.text_model = text_model\n        self.image_model = image_model\n        self.fc = nn.Linear(NUM_CLASSES * 2, NUM_CLASSES)\n\n    def forward(self, text, image):\n        text_out = self.text_model(text)\n        image_out = self.image_model(image)\n        \n        # Element-wise multiplication of text and image features\n        fusion = text_out * image_out\n        \n        # Flatten the fusion tensor\n        fusion = fusion.view(fusion.size(0), -1)\n        \n        out = self.fc(fusion)\n        return out\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3. **Dataset and DataLoader**","metadata":{}},{"cell_type":"markdown","source":"The dataset is a combination of text and image data, which is processed using word embeddings for the text and a pre-trained image classifier for the image features. Here’s how the dataset is loaded and split:","metadata":{}},{"cell_type":"code","source":"import multiprocessing as mp\ndata = MultimodalDataset(DF, W2V_MODEL)\n\nindices = np.arange(len(data))\ntrain_indices, test_indices = train_test_split(indices, test_size=0.2, random_state=42)\n\ntrain_data = Subset(data, train_indices)\ntest_data = Subset(data, test_indices)\n\nBATCH_SIZE = 2048\n\n# create data loaders for train and test sets\ntrain_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\ntest_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 4. **Model Training**\n","metadata":{}},{"cell_type":"markdown","source":"The model is trained using the Adam optimizer and Cross-Entropy loss:","metadata":{}},{"cell_type":"code","source":"text_model = TextClassifier().to(DEVICE)\nimage_model = ImageClassifier().to(DEVICE)\n\nmodel = MultimodalClassifier(text_model, image_model).to(DEVICE)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=LR)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Reflections\n\n### What surprised me?\n- The model could still capture emotions even when either the image or the text was unclear on its own — the fusion really helped!\n- Looking at the attention maps was super insightful — they showed cool connections, like linking smiley faces in images to positive words in the text.\n\n### Scope for improvement\n- It’d be exciting to extend this to a **tri-modal setup** — adding audio alongside text and images.\n- I could try using stronger pretrained models like **CLIP** or **BLIP** for better representation.\n- Instead of just classifying sentiment (positive/neutral/negative), predicting **emotion intensity** as a regression task could make the model more nuanced.\n","metadata":{}},{"cell_type":"markdown","source":"## References\n\n- GitHub Repo: https://github.com/imadhou/multimodal-sentiment-analysis\n- Paper: https://arxiv.org/abs/2005.13907 (Multimodal Transformer for Sentiment Analysis)\n- HuggingFace Transformers: https://huggingface.co/docs/transformers/index\n- PyTorch Docs: https://pytorch.org/docs/stable/index.html\n","metadata":{}}]}